# ğŸ¤– GuÃ­a RÃ¡pida: ConfiguraciÃ³n de Proveedores LLM

Esta guÃ­a te ayuda a configurar y cambiar entre los diferentes proveedores de modelos de lenguaje (LLM) disponibles en **Snowflake NLP Agent v2.3**.

## ğŸ¯ Proveedores Disponibles

| Proveedor | Modelo | UbicaciÃ³n | Costo | Privacidad | Velocidad |
|-----------|--------|-----------|-------|------------|-----------|
| **ğŸ  Ollama** | CodeLlama 7B | Local | ğŸ’š **Gratis** | ğŸ”’ **MÃ¡xima** | âš¡ **RÃ¡pida** |
| **ğŸŸ¢ Gemini** | Gemini 1.5 Flash | Cloud | ğŸ’› **EconÃ³mico** | ğŸ”„ **Media** | âš¡ **Muy RÃ¡pida** |
| **ğŸ”µ Groq** | Llama 3.3 70B | Cloud | ğŸ§¡ **Moderado** | ğŸ”„ **Media** | ğŸš€ **Ultra RÃ¡pida** |

## ğŸš€ ConfiguraciÃ³n RÃ¡pida por Proveedor

### ğŸ  OpciÃ³n 1: Ollama (Modelo Local - RECOMENDADO para Privacidad)

```bash
# 1. Configurar variables en .env
OLLAMA_BASE_URL=http://localhost:11434  # O tu servidor Ollama
OLLAMA_MODEL=codellama:7b-instruct
LLM_PROVIDER=ollama

# 2. Verificar que Ollama estÃ© ejecutÃ¡ndose
curl http://localhost:11434/api/tags

# 3. Ejecutar aplicaciÃ³n
source venv/bin/activate && streamlit run streamlit_app.py --server.port 8502
```

**âœ… Ventajas:**
- ğŸ”’ **Privacidad total** - Sin datos enviados a internet
- ğŸ’° **Costo cero** - Sin gastos de API
- ğŸ›¡ï¸ **Offline** - Funciona sin conexiÃ³n a internet
- ğŸ¯ **Especializado** - CodeLlama optimizado para SQL

**âš ï¸ Requisitos:**
- Servidor Ollama configurado
- Modelo CodeLlama descargado
- Al menos 4GB RAM disponible

---

### ğŸŸ¢ OpciÃ³n 2: Gemini (Cloud - RECOMENDADO por Defecto)

```bash
# 1. Obtener API Key de Google AI Studio
# https://aistudio.google.com/app/apikey

# 2. Configurar variables en .env
GOOGLE_API_KEY=tu-google-api-key-aqui
GEMINI_MODEL=gemini-1.5-flash
LLM_PROVIDER=gemini

# 3. Ejecutar aplicaciÃ³n
source venv/bin/activate && streamlit run streamlit_app.py --server.port 8502
```

**âœ… Ventajas:**
- âš¡ **Muy rÃ¡pido** - Respuesta casi instantÃ¡nea
- ğŸ’¡ **Inteligente** - Excelente comprensiÃ³n de contexto
- ğŸ’° **EconÃ³mico** - Precios competitivos
- ğŸŒ **Reliable** - Alta disponibilidad

---

### ğŸ”µ OpciÃ³n 3: Groq (Cloud - Ultra Velocidad)

```bash
# 1. Obtener API Key de Groq Console
# https://console.groq.com/keys

# 2. Configurar variables en .env
GROQ_API_KEY=gsk_tu-groq-api-key-aqui
MODEL_NAME=llama-3.3-70b-versatile
LLM_PROVIDER=groq

# 3. Ejecutar aplicaciÃ³n
source venv/bin/activate && streamlit run streamlit_app.py --server.port 8502
```

**âœ… Ventajas:**
- ğŸš€ **Ultra rÃ¡pido** - Inferencia extremadamente veloz
- ğŸ§  **Potente** - Modelo Llama 70B parÃ¡metros
- ğŸ¯ **Preciso** - Excelente para SQL complejo

---

## âš™ï¸ Auto-DetecciÃ³n Inteligente

Si configuraste mÃºltiples proveedores, usa **auto-detecciÃ³n** para selecciÃ³n automÃ¡tica:

```bash
# En .env
LLM_PROVIDER=auto

# Prioridad automÃ¡tica: Ollama > Gemini > Groq
# (Prioriza modelos locales por privacidad)
```

## ğŸ”„ Cambio RÃ¡pido de Proveedores

### Durante Desarrollo:
```bash
# Cambiar a Ollama
export LLM_PROVIDER=ollama
streamlit run streamlit_app.py

# Cambiar a Gemini
export LLM_PROVIDER=gemini
streamlit run streamlit_app.py

# Cambiar a Groq
export LLM_PROVIDER=groq
streamlit run streamlit_app.py
```

### Para ProducciÃ³n:
```bash
# Editar .env y reiniciar
nano .env
# Cambiar: LLM_PROVIDER=gemini
# Reiniciar aplicaciÃ³n
```

## ğŸ“Š ComparaciÃ³n de Rendimiento

### Para SQL Simple (ej: "lista clientes"):
| Proveedor | Tiempo Promedio | PrecisiÃ³n | RecomendaciÃ³n |
|-----------|----------------|-----------|---------------|
| **Ollama** | ~3-5s | 95% | ğŸ  **Uso personal/privado** |
| **Gemini** | ~1-2s | 98% | ğŸ¢ **ProducciÃ³n general** |
| **Groq** | ~0.5-1s | 97% | âš¡ **Aplicaciones tiempo real** |

### Para SQL Compleja (ej: "top 10 clientes por regiÃ³n"):
| Proveedor | Tiempo Promedio | PrecisiÃ³n | RecomendaciÃ³n |
|-----------|----------------|-----------|---------------|
| **Ollama** | ~5-8s | 90% | ğŸ  **Casos no crÃ­ticos** |
| **Gemini** | ~2-3s | 95% | ğŸ¢ **Balanceado** |
| **Groq** | ~1-2s | 98% | ğŸš€ **AnÃ¡lisis avanzado** |

## ğŸ› SoluciÃ³n de Problemas

### Ollama no conecta:
```bash
# Verificar estado
ollama list
systemctl status ollama  # Linux
brew services list | grep ollama  # macOS

# Reiniciar servicio
sudo systemctl restart ollama  # Linux
brew services restart ollama  # macOS
```

### Error API Key Gemini:
```bash
# Verificar key
curl -X POST \
  -H "Content-Type: application/json" \
  -d '{"contents":[{"parts":[{"text":"test"}]}]}' \
  "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=TU_API_KEY"
```

### Error API Key Groq:
```bash
# Verificar key
curl -X POST \
  -H "Authorization: Bearer gsk_TU_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{"messages":[{"role":"user","content":"test"}],"model":"llama-3.3-70b-versatile"}' \
  "https://api.groq.com/openai/v1/chat/completions"
```

## ğŸ’¡ Recomendaciones por Escenario

### ğŸ  **Uso Personal/Desarrollo:**
- **Ollama** - Privacidad y costo cero
- ConfiguraciÃ³n: `LLM_PROVIDER=ollama`

### ğŸ¢ **ProducciÃ³n Empresarial:**
- **Gemini** - Balance precio/rendimiento/confiabilidad
- ConfiguraciÃ³n: `LLM_PROVIDER=gemini`

### âš¡ **Aplicaciones Tiempo Real:**
- **Groq** - MÃ¡xima velocidad de inferencia
- ConfiguraciÃ³n: `LLM_PROVIDER=groq`

### ğŸ”’ **Datos Sensibles:**
- **Ollama** - Procesamiento 100% local
- ConfiguraciÃ³n: `LLM_PROVIDER=ollama`

### ğŸŒ **Multi-RegiÃ³n:**
- **Auto-detecciÃ³n** - Failover automÃ¡tico
- ConfiguraciÃ³n: `LLM_PROVIDER=auto`

---

## ğŸ“‹ Lista de VerificaciÃ³n Pre-Despliegue

### âœ… ConfiguraciÃ³n MÃ­nima:
- [ ] Al menos un proveedor LLM configurado
- [ ] Variables de entorno validadas
- [ ] ConexiÃ³n Snowflake funcionando
- [ ] Dependencias instaladas (`pip install -r requirements.txt`)

### âœ… Para Ollama Local:
- [ ] Servidor Ollama ejecutÃ¡ndose
- [ ] Modelo CodeLlama descargado
- [ ] Red accesible (si servidor remoto)
- [ ] Recursos suficientes (RAM/CPU)

### âœ… Para Gemini:
- [ ] API Key vÃ¡lida de Google AI Studio
- [ ] LÃ­mites de cuota configurados
- [ ] Billing habilitado (si necesario)

### âœ… Para Groq:
- [ ] API Key vÃ¡lida de Groq Console
- [ ] LÃ­mites de rate entendidos
- [ ] Backup provider configurado

---

**ğŸš€ Â¡Listo para usar! Tu Snowflake NLP Agent v2.3 estÃ¡ configurado con el proveedor LLM Ã³ptimo para tu caso de uso.**