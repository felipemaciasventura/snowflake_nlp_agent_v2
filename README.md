# ğŸ¤– Snowflake NLP Agent v2

An intelligent web application built with Streamlit that enables natural
language (English) queries to Snowflake databases, using LangChain with
**triple support** for Groq/Llama, Google Gemini, and Ollama (local
model) for automatic text-to-SQL conversion with hybrid query detection.

![License](https://img.shields.io/badge/license-MIT-blue.svg)
![Python](https://img.shields.io/badge/python-3.8+-blue.svg)
![Streamlit](https://img.shields.io/badge/streamlit-1.28+-red.svg)
![Snowflake](https://img.shields.io/badge/snowflake-supported-blue.svg)

## ğŸŒŸ Key Features

-   **ğŸ’¬ Intuitive Chat Interface**: Natural conversation with your
    database\
-   **ğŸ§  Hybrid NLP Processing**: Intelligent query detection (DB vs
    help vs off-topic)\
-   **ğŸ”„ Triple LLM Support**: Compatible with Groq/Llama, Google
    Gemini, and Ollama (local) with auto-detection\
-   **ğŸ“Š Smart Visualization**: Automatic result formatting with
    interactive tables\
-   **ğŸ”’ Secure Connection**: Robust integration with Snowflake using
    encrypted credentials\
-   **ğŸ¯ Educational Responses**: Intelligent guidance for users with
    examples and friendly redirection\
-   **ğŸ¨ Modern Interface**: Responsive design with Streamlit and
    interactive components

## ğŸš€ Quick Start

### Prerequisites

-   Python 3.8+\
-   Snowflake account with access credentials\
-   **Groq API Key** (option 1) for Llama models\
-   **Google Gemini API Key** (option 2) for Gemini models\
-   **Ollama Server** (option 3) for local models\
-   At least one of the three LLM providers configured

### 1. Installation

``` bash
# Clone repository
git clone https://github.com/your-user/snowflake_nlp_agent_v2.git
cd snowflake_nlp_agent_v2

# Create and activate virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scriptsctivate   # Windows

# Install dependencies
pip install -r requirements.txt
```

### 2. Configuration

``` bash
# Copy configuration template
cp .env.example .env

# Edit .env with your credentials
nano .env
```

Configure the following variables in `.env`:

``` env
# Snowflake Credentials
SNOWFLAKE_ACCOUNT=your-account-url
SNOWFLAKE_USER=your-user
SNOWFLAKE_PASSWORD=your-password
SNOWFLAKE_WAREHOUSE=your-warehouse
SNOWFLAKE_DATABASE=your-database
SNOWFLAKE_SCHEMA=PUBLIC

# LLM Providers - Configure at least one
# Groq (option 1)
GROQ_API_KEY=your-groq-api-key
MODEL_NAME=llama-3.3-70b-versatile

# Google Gemini (option 2) - RECOMMENDED
GOOGLE_API_KEY=your-google-api-key
GEMINI_MODEL=gemini-1.5-flash

# Ollama (option 3 - local model)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=codellama:7b-instruct

# Provider selection (auto, groq, gemini, ollama)
LLM_PROVIDER=gemini

# Optional
DEBUG=False
```

### 3. Run Application

``` bash
# Activate virtual environment
source venv/bin/activate

# Run application
streamlit run streamlit_app.py
```

The application will be available at `http://localhost:8501`

## ğŸ’» Usage Examples

### ğŸ” Database Queries

    ğŸ”¹ "What are the 10 orders with the highest value?"
    ğŸ”¹ "Show me sales for this month"
    ğŸ”¹ "How many customers are there in total?"
    ğŸ”¹ "List the best-selling products"
    ğŸ”¹ "What database am I using?"
    ğŸ”¹ "Show available tables"
    ğŸ”¹ "What is the average revenue per region?"

### ğŸ¯ Help Queries (Educational Response)

    ğŸ”¹ "How can you help me?"
    ğŸ”¹ "What can you do?"
    ğŸ”¹ "How does this application work?"
    ğŸ”¹ "Show me examples of what you can do"

### ğŸš« Off-Topic Queries (Friendly Redirection)

    ğŸ”¹ "How's the weather?"
    ğŸ”¹ "Tell me a joke"
    ğŸ”¹ "What movies do you recommend?"
    â†’ Friendly redirection to DB functionalities

### Automatic Results

The application automatically generates: - âœ… **Optimized and validated
SQL queries**\
- ğŸ“Š **Formatted tables** with user-friendly column names\
- ğŸ’° **Currency formatting** for financial values\
- ğŸ“ˆ **Record counters** and statistics\
- ğŸ” **Persistent conversation history**

## ğŸ—ï¸ Architecture

### Project Structure

    snowflake_nlp_agent_v2/
    â”œâ”€â”€ ğŸ“„ streamlit_app.py         # Main application
    â”œâ”€â”€ ğŸ“ src/
    â”‚   â”œâ”€â”€ ğŸ¤– agent/              # NLP logic and LangChain
    â”‚   â”‚   â””â”€â”€ nlp_agent.py
    â”‚   â”œâ”€â”€ ğŸ—„ï¸  database/           # Snowflake connection
    â”‚   â”‚   â””â”€â”€ snowflake_conn.py
    â”‚   â””â”€â”€ âš™ï¸  utils/              # Configuration and helpers
    â”‚       â”œâ”€â”€ config.py
    â”‚       â””â”€â”€ helpers.py
    â”œâ”€â”€ ğŸ“‹ requirements.txt        # Python dependencies
    â”œâ”€â”€ ğŸ”§ .env.example           # Configuration template
    â””â”€â”€ ğŸ“š WARP.md                # Development documentation

### Key Technologies

  Technology          Purpose                       Version
  ------------------- ----------------------------- ----------------
  **Streamlit**       Web framework                 1.28+
  **LangChain**       LLM orchestration             0.1+
  **Groq**            LLM API (Llama 3.3) âœ…        Latest
  **Google Gemini**   LLM API (Gemini 1.5) âœ…       Latest
  **Ollama**          Local models (CodeLlama) âœ…   0.6+
  **Snowflake**       Data warehouse                Connector 3.0+
  **Pandas**          Data manipulation             1.5+
  **SQLAlchemy**      ORM and connections           2.0+

## ğŸ”§ Advanced Configuration

### Environment Variables

  ------------------------------------------------------------------------------------------
  Variable                Description           Required         Example
  ----------------------- --------------------- ---------------- ---------------------------
  `SNOWFLAKE_ACCOUNT`     Snowflake account URL âœ…               `your-org-account`

  `SNOWFLAKE_USER`        Snowflake user        âœ…               `user@company.com`

  `SNOWFLAKE_PASSWORD`    User password         âœ…               `password123`

  `SNOWFLAKE_WAREHOUSE`   Warehouse to use      âœ…               `COMPUTE_WH`

  `SNOWFLAKE_DATABASE`    Database              âœ…               `PROD_DB`

  `SNOWFLAKE_SCHEMA`      Default schema        âŒ               `PUBLIC`

  `GROQ_API_KEY`          Groq API key (option  ğŸ”„               `gsk_...`
                          1)                                     

  `GOOGLE_API_KEY`        Google Gemini API key ğŸ”„               `AIza...`
                          (option 2)                             

  `OLLAMA_BASE_URL`       Ollama server URL     ğŸ”„               `http://localhost:11434`
                          (option 3)                             

  `OLLAMA_MODEL`          Ollama model          âŒ               `codellama:7b-instruct`

  `MODEL_NAME`            Groq model            âŒ               `llama-3.3-70b-versatile`

  `GEMINI_MODEL`          Gemini model          âŒ               `gemini-1.5-flash`

  `LLM_PROVIDER`          Provider selection    âŒ               `auto`, `groq`, `gemini`,
                                                                 `ollama`
  ------------------------------------------------------------------------------------------

**Note:** ğŸ”„ = At least one of the three LLM providers must be
configured

### Development Commands

``` bash
# Run with specific port
streamlit run streamlit_app.py --server.port 8080

# Development mode with detailed logs
DEBUG=True streamlit run streamlit_app.py

# Production (public server)
streamlit run streamlit_app.py --server.port 8080 --server.address 0.0.0.0

# Syntax check
python -m py_compile streamlit_app.py

# Linting
flake8 src/ streamlit_app.py
```

## ğŸ”¬ Detailed Workflow Example

To understand how the magic works behind the scenes, let's follow the
journey of a simple question through the system.

**User Question:** `Which are the 10 customers who have spent the most?`

------------------------------------------------------------------------

#### **Step 1: User Interface (Streamlit)**

1.  **User Input**: The user types the question into the web app chat
    (`streamlit_app.py`).\
2.  **Input Processing**: The app immediately saves and displays the
    user's message in the interface.\
3.  **Agent Call**: The system's core is invoked:
    `agent.process_query(...)`.

------------------------------------------------------------------------

#### **Step 2: NLP Agent Layer (LangChain + Groq)**

4.  **Start of Processing**: The `SnowflakeNLPAgent`
    (`src/agent/nlp_agent.py`) receives the query.\
5.  **Prompt Construction**: LangChain's `SQLDatabaseChain` combines the
    user's question with the database schema and a prompt template.\
6.  **LLM Invocation**: The full prompt is sent to the Groq API, using
    the `llama-3.3-70b-versatile` model.\
7.  **SQL Generation**: Guided by the prompt, the LLM generates the
    corresponding SQL query.\
    `sql     SELECT c.c_name, SUM(o.o_totalprice) AS total_spent     FROM CUSTOMER c     JOIN ORDERS o ON c.c_custkey = o.o_custkey     GROUP BY c.c_name     ORDER BY total_spent DESC     LIMIT 10`
8.  **SQL Extraction**: The agent extracts the generated SQL from
    LangChain's response.

------------------------------------------------------------------------

#### **Step 3: Data Access Layer (Snowflake)**

9.  **Query Execution**: The agent executes the SQL query via the
    database connection layer (`src/database/snowflake_conn.py`).\
10. **Snowflake Processing**: Snowflake executes the query in its
    compute engine and returns results. Example:\
    `[('Customer#0001', Decimal('555285.16')), ('Customer#0002', Decimal('544089.09')), ...]`
11. **Result Reception**: The application receives these results (a list
    of tuples).

------------------------------------------------------------------------

#### **Step 4: Formatting and Visualization (Streamlit)**

12. **Smart Formatting**: A utility function
    (`format_sql_result_to_dataframe`) converts the tuple list into a
    Pandas DataFrame, applying currency formatting and user-friendly
    column names.\
13. **Final Visualization**:\

-   Displays the formatted DataFrame in an interactive table\
-   Shows a counter below the table: `ğŸ“Š 10 records found`\
-   Saves the full response in the chat history

------------------------------------------------------------------------

#### **Step 5: Traceability (Logs in UI)**

14. **Logs Panel**: Throughout the process, detailed logs are recorded
    and shown in the sidebar, providing full transparency---from the SQL
    that was generated to the results obtained.

## ğŸ”„ Recent Updates (v2.3)

### âœ… New Key Features

-   **ğŸ› ï¸ Ollama Support**: Full integration for local models (CodeLlama
    7B-Instruct)\
-   **ğŸ”„ Triple LLM Support**: Groq/Llama + Google Gemini + Ollama with
    auto-detection and local priority\
-   **ğŸ“ Advanced SQL Cleaning**: Robust system to handle CodeLlama
    markdown formatting\
-   **ğŸ  Local Processing**: Full privacy option with a local model, no
    API costs

### âœ… Updates v2.2

-   **ğŸ§  Hybrid Detection**: Smart classification of queries (DB vs help
    vs off-topic)\
-   **ğŸ¯ Educational Responses**: Complete guidance with examples for
    new users\
-   **ğŸš€ Friendly Redirection**: Polite responses for off-topic queries\
-   **ğŸ“Š Dynamic Info**: Sidebar shows active LLM model in real time

### âœ… Previous Improvements (v2.1)

-   **ğŸ¯ Smart Formatting**: Automatic recognition of query types\
-   **ğŸ’¹ Currency Formatting**: Automatic display of financial values\
-   **ğŸ”§ Robust Parsing**: Advanced handling of Snowflake Decimal
    objects\
-   **âš¡ Updated Models**: Llama 3.3 70B Versatile + Gemini 1.5 Flash\
-   **ğŸ–¥ï¸ Improved UI**: Full-width tables and record counters

### ğŸ› Bug Fixes

-   âœ… Deprecated `__call__` replaced by `invoke`\
-   âœ… Robust error handling in DataFrame constructor\
-   âœ… Parsing of strings with complex SQL results\
-   âœ… Dynamic LLM provider configuration\
-   âœ… Automatic detection of available models

## ğŸ¤ Contribution

1.  **Fork** the project\
2.  Create a feature branch (`git checkout -b feature/AmazingFeature`)\
3.  Commit changes (`git commit -m 'Add AmazingFeature'`)\
4.  Push to branch (`git push origin feature/AmazingFeature`)\
5.  Open a **Pull Request**

## ğŸ“ License

This project is licensed under the MIT License. See `LICENSE` for
details.

## ğŸ†˜ Support

Problems or questions?

-   ğŸ“§ **Email**: support@company.com\
-   ğŸ› **Issues**: [GitHub
    Issues](https://github.com/your-user/snowflake_nlp_agent_v2/issues)\
-   ğŸ“š **Documentation**: See `WARP.md` for technical details

## ğŸ™ Acknowledgments

-   **Streamlit** for the amazing web framework\
-   **LangChain** for LLM orchestration\
-   **Groq** for fast LLM services\
-   **Snowflake** for the robust data platform

------------------------------------------------------------------------

**Built with â¤ï¸ using Python and modern AI technologies**
